[{"categories":["task-automation"],"contents":"Shell scripts are powerful tools that allow you to automate various tasks on Unix and Linux systems. By encapsulating a series of commands in a single script, it is possible to streamline processes, reduce errors and increase operational efficiency. This versatile programming language makes it easy to manipulate files, automate administrative tasks and provide precise control over the system, making it ideal for a wide range of applications.\nUsing the knowledge acquired in the course Hands-on Introduction to Linux Commands and Shell Scripting, I applied the project to a real-world scenario, Boutique Manager, which consists of a shell script management system for small clothing stores.\nThe system provides an integrated set of essential functions for managing a clothing store, starting with product registration, which allows you to record detailed information by category and manage stock automatically. The sales module processes transactions in real time, updating stock levels and recording information such as date, time and value. In addition, a stock monitoring system proactively alerts you to low product levels, facilitating replenishment. Sales reports provide comprehensive analysis of business performance, including totals by category and period.\nData security and integrity is ensured by an automated backup system that protects critical information on a daily basis, and a robust logging system that records all operations for audit purposes. The system performs validations on critical operations, such as checking available stock before selling, and uses temporary files to maintain integrity during updates. All of this functionality is accessible through an intuitive menu that organises operations logically, allowing users with different levels of experience to operate the system efficiently, resulting in more effective and secure store management.\nNote: Open the image in a new browser tab to get the zoom you need.\nThe shell script management project uses several essential techniques, including environment variables such as LOJA_DIR and DATA, defined with the export command, to facilitate environment configuration. Data manipulation is optimised through the use of pipes and filters such as awk, sort and uniq, allowing relevant statistics to be generated. Meta characters such as * for file patterns and $(\u0026hellip;) for command substitution are also used to increase script flexibility. I/O redirection is used to record logs and create new files, ensuring efficient data management.\nModularising the code through functions improves readability and maintainability, while passing parameters allows greater reusability. Conditional structures such as if/then/else and case are used to manage the system menu, complemented by while loops that facilitate user interaction and iteration over arrays such as categories. Task scheduling is managed by cron, which ensures automatic backups and weekly inventory checks, contributing to the robustness and reliability of the system. Data security is provided by atomic operations using temporary files during updates, while a record of all operations is maintained through time-stamped logs using the date command.\nThe benefits of scripting automation are enormous. By automating repetitive tasks, you can save time, reduce the risk of human error and ensure process consistency. In addition, scripts can be used to monitor systems, generate reports and perform backups, providing greater security and reliability. In an increasingly digital world, scripting automation is becoming a competitive differentiator for organisations seeking to optimise processes and achieve operational excellence.\nFor more details, to implement this project, or to make suggestions and improvements, visit the repository link below.\nProject repository https://github.com/diogo-dantas/boutique-manager\nDesigned by gstudioimagen / Freepik\n","date":"October 31, 2024","hero":"/posts/task-automation/shell-script/boutique-manager/images/post2.jpg","permalink":"https://diogo-dantas.github.io/posts/task-automation/shell-script/boutique-manager/","summary":"\u003cp\u003eShell scripts are powerful tools that allow you to automate various tasks on Unix and Linux systems. By encapsulating a series of commands in a single script, it is possible to streamline processes, reduce errors and increase operational efficiency. This versatile programming language makes it easy to manipulate files, automate administrative tasks and provide precise control over the system, making it ideal for a wide range of applications.\u003c/p\u003e\n\u003cp\u003eUsing the knowledge acquired in the course \u003ca href=\"https://www.coursera.org/account/accomplishments/verify/30JGE2RZY8T4\" title=\"Visit the course certificate\" target=\"_blank\" rel=\"noopener\"\u003eHands-on Introduction to Linux Commands and Shell Scripting\u003c/a\u003e, I applied the project to a real-world scenario, Boutique Manager, which consists of a shell script management system for small clothing stores.\u003c/p\u003e","tags":["Cron","Shell Script","Bash (Unix Shell)"],"title":"Intelligent management: Shell script integration and task automation"},{"categories":["Data Engineer"],"contents":"In today\u0026rsquo;s dynamic business environment, integrating and analysing data from multiple sources is essential for making informed decisions. In many industries, such as finance, logistics or marketing, it is common to receive data files in a variety of formats, including CSV, XML and JSON. These files can contain valuable information, but their dispersion and variety of formats make processing and analysis a significant challenge.\nOne of the challenges in this scenario is the need to consolidate information from multiple files into a single format that can be easily imported into a database, for example. The task becomes even more critical when considering the need to maintain data integrity and consistency during this compilation process.\nTo illustrate a simple and effective solution to this challenge, I have used as an example files containing name, height and weight information available in a central directory dedicated to this purpose. The data is stored in various formats: CSV, JSON and XML.\nETL is a fundamental process in data management and system integration. Through this process, using the Python language and libraries: glob, pandas, datetime and ElementTree, information is extracted from the files, transforming weight from inches to kg and height from pounds to metres according to defined business rules. The compiled data is returned in a csv file called transformed_data.csv. Another fundamental output file to store all the actions performed during the ETL process will be the log_file.txt file.\nThe importance of a log file in an ETL process lies in its ability to provide a complete trace of all activities performed. By recording each step in detail, the log allows teams to monitor execution in real time, quickly identify and resolve problems, optimise process performance, comply with regulatory requirements and document all actions for future reference. In addition, regular analysis of the log allows you to identify opportunities for continuous improvement, ensuring that the ETL process is always efficient and effective.\nThe video below, in Portuguese, provides a summary of the proposed solution for the project.\nWe can imagine other scenarios where a company receives daily sales, stock and customer feedback reports in different formats. In order to optimise analysis and reporting, it is essential to consolidate all this information into a single CSV file.\nIn short, data aggregation is a critical process for transforming scattered information into useful knowledge. By automating and optimising this process, organisations can extract maximum value from their data and drive growth and competitiveness.\nProject repository https://github.com/diogo-dantas/ETL-simplificado-CSV-JSON-e-XML-\nDesigned by gstudioimagen / Freepik\n","date":"September 20, 2024","hero":"/posts/etl/python/etl-simplificado/images/post1.jpg","permalink":"https://diogo-dantas.github.io/posts/etl/python/etl-simplificado/","summary":"\u003cp\u003eIn today\u0026rsquo;s dynamic business environment, integrating and analysing data from multiple sources is essential for making informed decisions. In many industries, such as finance, logistics or marketing, it is common to receive data files in a variety of formats, including CSV, XML and JSON. These files can contain valuable information, but their dispersion and variety of formats make processing and analysis a significant challenge.\u003c/p\u003e\n\u003cp\u003eOne of the challenges in this scenario is the need to consolidate information from multiple files into a single format that can be easily imported into a database, for example. The task becomes even more critical when considering the need to maintain data integrity and consistency during this compilation process.\u003c/p\u003e","tags":["ETL","Python"],"title":"ETL simplified: unified data"}]